{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import random\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import TEXT\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browse(url, proxy = None):\n",
    "    #     url = 'https://httpbin.org/ip'\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    if proxy is not None:\n",
    "        chrome_options.add_argument('--proxy-server=%s' % proxy)\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    driver = webdriver.Chrome('/home/ubuntu/chromedriver',chrome_options=chrome_options)\n",
    "    driver.get(url)\n",
    "    driver.implicitly_wait(3) # seconds\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.close()\n",
    "    return page_source, soup\n",
    "    \n",
    "class CrawlMKListing():\n",
    "    MONGO_URI = \"localhost:27017\"\n",
    "    MONGO_DB = \"news\"\n",
    "    MONGO_COLLECTION = \"malaysiakini_v1_test1\"\n",
    "    DOMAIN = \"https://www.malaysiakini.com\"\n",
    "    NEWS_LISTING_DOMAIN = \"https://www.malaysiakini.com/stories/covid19\"\n",
    "    PROXY_LIST = [\"socks4://120.50.56.137:40553\",\"socks4://121.122.50.157:4145\", \n",
    "                  \"socks4://1.9.167.36:60489\",\"socks4://1.9.111.145:4145\",\n",
    "                  \"socks4://45.117.228.153:4145\",\"socks4://45.117.228.97:4145\",\n",
    "                  \"socks4://103.220.6.254:4145\"\n",
    "                 ]\n",
    "    START_URL = NEWS_LISTING_DOMAIN\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def crawl(self):\n",
    "        CURRENT_PAGE_URL = self.START_URL\n",
    "        self.init_mongo()\n",
    "        proxy = random.sample(self.PROXY_LIST,1)[0]\n",
    "        print(\"Using proxy:\", proxy)\n",
    "        page_source, soup = browse(url = CURRENT_PAGE_URL, proxy = proxy)\n",
    "        _check = self.check(soup)\n",
    "        # if _check is not True, retry the whole crawl for this page\n",
    "        if _check: \n",
    "            to_insert, next_page = self.parse(soup)\n",
    "            to_insert_2 = self.prevent_duplicate(to_insert)\n",
    "            if len(to_insert_2) > 0:\n",
    "                self.coll.insert_many(to_insert_2)\n",
    "        else:\n",
    "            print(\"False check.\")\n",
    "            \n",
    "    def init_mongo(self):\n",
    "        client = pymongo.MongoClient(self.MONGO_URI)\n",
    "        self.coll = client[self.MONGO_DB][self.MONGO_COLLECTION]\n",
    "\n",
    "    def check(self, soup):\n",
    "        try:\n",
    "            x = soup.find(\"title\").get_text()\n",
    "        except:\n",
    "            return False\n",
    "        if x is None:\n",
    "            return False\n",
    "        if x.find(\"Access denied\") >= 0:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def parse(self, soup):\n",
    "        DOMAIN = self.DOMAIN\n",
    "        NEWS_LISTING_DOMAIN = self.NEWS_LISTING_DOMAIN\n",
    "        \n",
    "        x = soup.find(\"div\", \"news\").find_all(\"a\")[:-1]\n",
    "        titles = [j.find(\"h3\").getText() for j in x]\n",
    "        urls = [j.get(\"href\") for j in x]\n",
    "        urls = [DOMAIN + url.replace(DOMAIN, \"\") for url in urls]\n",
    "        df = pd.DataFrame(dict(title = titles, url = urls)) #.sample(30)\n",
    "        if df.shape[0] == 0:\n",
    "            raise Exception(\"shape 0\")\n",
    "        to_insert = df.to_dict(orient=\"records\")\n",
    "        next_page = NEWS_LISTING_DOMAIN + soup.find(\"div\", \"news\").find_all(\"a\")[-1].get(\"href\")\n",
    "        return to_insert, next_page\n",
    "    \n",
    "    def prevent_duplicate(self, to_insert):\n",
    "        url_list = [j.get(\"url\") for j in to_insert]\n",
    "        exist_url_list = [j.get(\"url\") for j in list(self.coll.find({\"url\": {\"$in\": url_list}}, {\"url\":1}))]\n",
    "        to_insert_2 = [j for j in to_insert if j.get(\"url\") not in exist_url_list]\n",
    "        return to_insert_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using proxy: socks4://103.220.6.254:4145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/covid/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: use options instead of chrome_options\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "C = CrawlMKListing()\n",
    "C.crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
