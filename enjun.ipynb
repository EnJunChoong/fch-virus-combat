{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "url='https://sebenarnya.my/anggota-pdrm-terlibat-dalam-rusuhan-kuil-sri-maha-mariamman-subang-jaya/'\n",
    "page = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PALSU:',\n",
       " 'Tular di media sosial satu klip video dan gambar yang mendakwa kononnya seorang anggota PDRM terlibat dalam rusuhan kuil Sri Maha Mariamman, Subang Jaya.',\n",
       " 'Sumber gambar: Facebook',\n",
       " 'SEBENARNYA:',\n",
       " 'Ketua Polis Negara, Tan Sri Dato Sri Mohamad Fuzi Harun menafikan penglibatan anggotanya dalam rusuhan Kuil Sri Maha Mariaman, Seafield, pada pagi Isnin di USJ 25, Putra Heights, Subang Jaya.',\n",
       " 'Beliau menjelaskan video dan gambar individu yang tular di media sosial berpakaian uniform polis yang dikaitkan dengan rusuhan kuil baru-baru ini adalah tidak benar.',\n",
       " '“Itu salah, dia bukan polis, kami dah buat semakan dan hasil, individu terbabit bukan anggota polis, dia hanyalah polis sukarelawan simpanan atau PVR yang telah lama dibuang perkhidmatan.”',\n",
       " 'SUMBER:',\n",
       " 'Facebook Rasmi PDRM',\n",
       " 'MAINKAN PERANAN ANDA!',\n",
       " 'Sekiranya anda mempunyai maklumat mengenai berita tidak ditentusah yang melibatkan kepentingan awam mahupun negara,',\n",
       " 'Salurkan Kepada Kami',\n",
       " 'TIDAK PASTI JANGAN KONGSI']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "x = soup.find('div',{'class':'td-post-content'})\n",
    "cleanLine = [line.strip() for line in x.text.split('\\n') if line.strip() != '']\n",
    "cleanLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "786"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with audio\n",
    "https://sebenarnya.my/dakwaan-nota-suara-anggota-atm-pukul-individu-yang-keluar-rumah-ketika-pkp-adalah-palsu/\n",
    "    \n",
    "#with caption\n",
    "https://sebenarnya.my/kilang-assb-di-taman-perindustrian-malaysia-china-kuantan-mckip-terbakar/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Processing:\n",
    "    def __init__(self, db_dict:dict):\n",
    "        '''take input of dict retrieved from mongoDB\n",
    "        '''\n",
    "        self.raw = db_dict \n",
    "        self.date = db_dict['date']\n",
    "        self.url =  db_dict['url']\n",
    "        self.title = self.raw['title']\n",
    "        self.category = 'COVID-19'\n",
    "        self.content_text, self.content_lines, self.fact_src, self.content_html = self.parse_content()\n",
    "        self.label_map = self.get_label_map()\n",
    "        self.label, self.confidence = self.get_label_n_confidence()\n",
    "        self.audios, self.images = self.get_figures()\n",
    "        self.json_file = self.to_json()\n",
    "        \n",
    "    def get_label_map(self):\n",
    "        label_map = {\n",
    "            '1' : ['tidak benar(:|.|$)', 'palsu(:|.|$)'],\n",
    "            '2' : ['^waspada'],\n",
    "            '3' : ['penjelasan(:|.|$)', '^makluman'],\n",
    "        }\n",
    "        return label_map\n",
    "        \n",
    "        \n",
    "    def parse_content(self):\n",
    "        content = self.raw['content_html']\n",
    "        soup = BeautifulSoup(content[0], 'html.parser')\n",
    "        rm_index = (soup.text.find(soup.find('div',{'class':'awac-wrapper'}).text) \n",
    "                    if soup.find('div',{'class':'awac-wrapper'}) else len(soup.text))\n",
    "        all_text = soup.text[:rm_index]\n",
    "        lines = [line.strip() for line in all_text.split('\\n') if line.strip() != '']\n",
    "\n",
    "        r = re.compile(\"[A-Z]*:$\")\n",
    "        keys = list(filter(r.match, lines))\n",
    "        keys_gen = iter(keys)\n",
    "        text_dict = {}\n",
    "        old_key_index = 0\n",
    "        for i, key in enumerate(keys):\n",
    "            try:\n",
    "                new_key_index = lines.index(next(keys_gen))\n",
    "                if i == 0:\n",
    "                    if new_key_index == 0:\n",
    "                        try:\n",
    "                            new_key_index = lines.index(next(keys_gen))\n",
    "                            text_dict[key] = '\\n'.join(lines[1:new_key_index])\n",
    "                        except Exception as e:\n",
    "                            pass\n",
    "                    else:\n",
    "                        text_dict['free_text'] = '\\n'.join(lines[:new_key_index])\n",
    "                else:\n",
    "                    text_dict[key] = '\\n'.join(lines[old_key_index+1:new_key_index])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            old_key_index = new_key_index\n",
    "        if re.match(re.compile('^sumber:?'), key.lower()):\n",
    "            fact_src = []\n",
    "            for line in lines[old_key_index+1:]:\n",
    "                fact_src.append({\n",
    "                    'text' : line,\n",
    "                    'link' : ('' if soup.find('a', href=True, text=line) is None\n",
    "                              else soup.find('a', href=True, text=line)['href'])\n",
    "                })\n",
    "        else:\n",
    "            text_dict[key] = '\\n'.join(lines[old_key_index+1:])\n",
    "\n",
    "        return text_dict, lines, fact_src, str(soup)\n",
    "    \n",
    "    def get_label_n_confidence(self):\n",
    "        keyword_found = []\n",
    "        for key in self.label_map.keys():\n",
    "            for regex in self.label_map[key]:\n",
    "                if re.search(regex, self.title.lower()):\n",
    "                    keyword_found.append(key)\n",
    "                for line in self.content_lines:\n",
    "                    if re.search(regex, line.lower()):\n",
    "                        keyword_found.append(key)\n",
    "\n",
    "        if len(keyword_found)==0:\n",
    "            label = 1    # default label is 1\n",
    "            confidence = 3 # if nothing is found give lowest confidence\n",
    "        else:\n",
    "            counter = collections.Counter(keyword_found)\n",
    "            label = int(counter.most_common(1)[0][0])\n",
    "            confidence = 1 if len(np.unique(keyword_found))==1 else 2\n",
    "        return (label, confidence)\n",
    "    \n",
    "    def get_figures(self):\n",
    "        audios = []\n",
    "        images = []\n",
    "        for figure in soup.find_all('figure'):\n",
    "            if figure.find('audio') is not None:\n",
    "                audios.append({\n",
    "                    'src' : figure.find('audio').get('src'),\n",
    "                    'caption' : [] if figure.find('figcaption') is None else figure.find('figcaption')\n",
    "                })\n",
    "            if figure.find('img') is not None:\n",
    "                images.append({\n",
    "                    'src' : figure.find('img').get('src'),\n",
    "                    'caption' : [] if figure.find('figcaption') is None else figure.find('figcaption')\n",
    "                })\n",
    "        return audios, images\n",
    "    \n",
    "    def to_json(self):\n",
    "        json_file = dict(\n",
    "            date = self.date,\n",
    "            category = self.category,\n",
    "            url = self.url,\n",
    "            title = self.title,\n",
    "            content_text = self.content_text,\n",
    "            images = self.images,\n",
    "            audios = self.audios,\n",
    "            fact_src = self.fact_src,\n",
    "            label = self.label,\n",
    "            confidence = self.confidence,\n",
    "            content_html = self.content_html,\n",
    "        )\n",
    "        return json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sebenarnya_v2_proccessed2', 'sebenarnya_v2_proccessed1', 'sebenarnya_v2_test1', 'sebenarnya_v1_test2']\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import json\n",
    "from Processing.Processing import Processing\n",
    "\n",
    "client = pymongo.MongoClient()\n",
    "db = client[\"news\"]\n",
    "print(client['news'].list_collection_names())\n",
    "coll_raw = db['sebenarnya_v2_test1']\n",
    "coll_processed = db['sebenarnya_v2_proccessed1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict={}\n",
    "len(text_dict) >0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asdsadsa'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'asdsadsa:'\n",
    "a.strip(':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coll_processed.delete_many({})\n",
    "list(coll_processed.find())[111]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw_dict in list(coll_raw.find())[1:]:\n",
    "    cls = Processing(raw_dict)\n",
    "    coll_processed.insert_one(cls.json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://sebenarnya.my/4-nota-suara-yang-menggunakan-nama-mkn-dakwa-darurat-akan-diisytihar-adalah-palsu/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = coll_raw.find_one({'title':'4 Nota Suara Yang Menggunakan Nama MKN Dakwa Darurat Akan Diisytihar Adalah Palsu'})\n",
    "x['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(db_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dict = raw_dict\n",
    "\n",
    "title = db_dict['title']\n",
    "content = db_dict['content_html']\n",
    "soup = BeautifulSoup(content[0], 'html.parser')\n",
    "rm_index = soup.text.find(soup.find('div',{'class':'awac-wrapper'}).text) if soup.find('div',{'class':'awac-wrapper'}) else len(soup.text)\n",
    "all_text = soup.text[:rm_index]\n",
    "lines = [line.strip() for line in all_text.split('\\n') if line.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SEBENARNYA:', 'SUMBER:'] 4 Nota Suara Yang Menggunakan Nama MKN Dakwa Darurat Akan Diisytihar Adalah Palsu\n"
     ]
    }
   ],
   "source": [
    "cls = Processing(db_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4 Nota Suara Yang Menggunakan Nama MKN Dakwa Darurat Akan Diisytihar Adalah Palsu'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SEBENARNYA: Jabatan Kesihatan Negeri Terengganu ingin merujuk kepada satu kenyataan yang tular di aplikasi WhatsApp berkenaan kononnya terdapat 30 kes COVID-19 di Durian Burung, Kuala Terengganu berpunca daripada majlis tahlil kematian dan 30 orang ahli keluarga telah dikuarantin di Chendering. Pihak jabatan ini menafikan sepenuhnya kenyataan tersebut. Sehingga 3 April 2020, tiada kes COVID-19 yang dikesan di Kg Durian Burung, Kuala Terengganu. SUMBER: Facebook Rasmi JKNT'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(all_text.split('\\n')).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
